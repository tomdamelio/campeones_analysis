Semana que viene ->

Martes ->

- [x] Terminar de editar read_xdf /o bids compliance para asegurar que se este agregando bien
      la informacion de lso participantes a la metadata.
- [x] Una vez terminado eso (que ya lo pregunte en ctrl+l), fijarme a ver como incorporar el resto 
      de la data de los participantes. Primero, lo tengo que agregar en la sourcedata. Y luego
      lo tengo que sumar a la `./raw` de acuerdo a BIDS
 
Miercoles -> Hacer los pasos 1, 2 y 3 de "Fases y salidas" (@arquitectura_datos)
- [x] Localizar las marcas de inicio y fin de registro. 
      La primera marca no estaba, porque parece que se hacia justo al comenzar el registro y quedaba por fuera
      La ultima marca aparecia pero justo al limite del registro, asi que no la agregamos.
- [x] Confirmar que la estructura BIDS es correcta usando BIDS Validator
- [x] Sacar toda la parte de sesiones con nox, linters automaticos, etc al momento de commitear.
      Tengo que poder pushear sin pasar por todos los filtros que tengo ahora (fue mucha complejidad).
      Eliminar tambien del pipeline el paso por PR de GitHub. Dejar todo esto para milestones del proyecto nomas. 
- [x] Docuemntar y pushear todos los cambios
- [x] Conversar con Jero como seguir. Explicarle como generar los archivos de cada participante.
      Y que la idea seria generar un script que epochee de acuerdo a las marcas.

Jueves -> Preparacion de Reunion con Diego (preguntas a responder con modelos GLHMM, etc)

Viernes ->  Prerparcion de tareas para inspecci√≥n general en los registros de los sujetos 


Sabado -> Inspecci√≥n general en los registros de los sujetos (principalemente las marcas, para poder chunkear los datos)

## üÖ∞Ô∏è Fase A ¬∑ Construir el `events.tsv` inicial a partir de crear un archivo en `./scripts/preprocessing`

* [ ] **Localizar la planilla de √≥rdenes**

Vamos a empezar levantando el archivo 

* [x] **Filtrar filas relevantes**
  Conserva solo las que tengan `description` en `['fixation', 'calm_901', 'video', 'luminance', 'verbal_report', 'calm_902']` usando `df.query()`.

* [x] **Asignar duraciones**
  Crea el diccionario Python de onset y durations.
  Para eso, vamos a tomar como input el siguiente dict con start y end de cada evento.
  ```
     durations = {'video': [0:00:35, 0:03:18],
                  'video': [0:05:29, 0:09:04],
                  'video': [0:11:17, 0:13:13], 
                  'luminance': [0:15:32, 0:16:30]}
  ```

* [x] **Inicializar anotaciones ‚Äúblandas‚Äù**

  ```python
  onsets = np.zeros(len(df))
  annot  = mne.Annotations(onsets, df.description.map(durations),
                           df.description)
  raw.set_annotations(annot)
  ```

  `mne.Annotations` garantiza compatibilidad posterior con `events_from_annotations()`.

* [x] **Exportar a BIDS**
  Construye un `BIDSPath` que apunte a `data/raw/sub-XX/ses-vr/..._eeg.eeg` y llama:

  ```python
  mne_bids.write_raw_bids(raw, bids_path,
                          events=annot,
                          overwrite=True)
  ```

  Esto crea autom√°ticamente `*_events.tsv` y  `*_events.json` en derivatives

* [x] **Validar**
  Ejecuta en terminal:

  ```bash
  bids-validator data/derivatives
  ```

  Corrige cualquier *ERROR* o *WARNING* antes de continuar.

---

## üÖ±Ô∏è Fase B ¬∑ Ajustar onsets con AUDIO y PHOTO

* [x] **Cargar eventos existentes y raw original**
  
  ```python
  from mne_bids import read_raw_bids, BIDSPath
  
  # Ruta a los eventos generados
  events_path = BIDSPath(subject='16', session='vr', task='02',
                        run='003', suffix='events', desc='init', extension='.tsv',
                        root=deriv_root, check=False)
  
  events_df = pd.read_csv(events_path.fpath, sep='\t')
  
  # Cargar raw original SIN anotaciones
  raw = read_raw_bids(bids_path)
  ```

* [x] **Visualizar marcadores sensoriales vs eventos**

  ```python
  # Crear anotaciones desde events.tsv
  annot = mne.Annotations(onset=events_df['onset'],
                         duration=events_df['duration'],
                         description=events_df['trial_type'])
  
  # Visualizar alineaci√≥n
  raw.pick(['AUDIO', 'PHOTO']).set_annotations(annot).plot()
  ```
VIDEOS
('0:00:35', '0:03:18') -> 203,141 a 365,292
('0:05:29', '0:09:04') -> 497,074 a 713,234
('0:11:17', '0:13:13') -> 1006,567 a 1123,834
LUMINANCE
('0:15:32', '0:16:30') -> 1261,240 a 1321,293

Lunes -> Terminar el dia con todos los archivos con las anotaciones guardadas en derivatives.
Si hay tiempo, empezar al pipeline de preprocesamiento de EEG.

* [x] Correr `visualize_events.py`, guardar las 4 anotaciones en los tiempos que corresponden
      (ver numeros mas arriba).

* [x] Con otro script, verificar si eso se guardo bien (`verify_annotations.py`)

## ‚òëÔ∏è Mejorar el archivo `visualize_events` para que se haga el guardado en *derivatives/aligned\_events*

* [x] **BIDSPath completo con `desc-withann`** (la entidad `desc-` distingue esta versi√≥n del TSV ‚Äúprivilegiado‚Äù y evita alertas de validador):

  ```python
bids_path = BIDSPath(subject='16', session='vr', task='02', run='003',
                  suffix='eeg', extension='.vhdr', datatype='eeg', root=bids_root)

  ```

* [x] Graba el TSV con separador `\t` y sin √≠ndice.
* [x] **Copia el `.json` del TSV original** y a√±ade/actualiza:
  ```json
  {
    "Description": "Eventos alineados con se√±ales fisiol√≥gicas",
    "Sources": ["../aligned_events/../desc-init_events.tsv"],
    "OffsetApplied": "manual AUDIO/PHOTO (s)
  }
  ```

  Campos adicionales son permitidos en sidecars JSON. 

## ‚òëÔ∏èMejorar el archivo `visualize_events` en cuanto a sus metadatos de la carpeta derivada

* [x] Ejecuta una sola vez por carpeta:

  ```python
  mne_bids.make_dataset_description(
      path=aligned_root,
      name='aligned_events',
      dataset_type='derivative',
      generated_by=[{
          "Name": "W3_phaseB.py",
          "Description": "Alineaci√≥n manual de eventos AUDIO/PHOTO"
      }],
      source_datasets=[{
          "URL": str(events_path.fpath)  # ruta relativa al TSV base
      }]
  )
  ```

  Esto cumple con la obligaci√≥n de `dataset_description.json` propia del pipeline. 

## ‚òëÔ∏è Validaci√≥n final

* [x] Corre `bids-validator derivatives/aligned_events` y confirma que **no aparezca** la advertencia *‚ÄúFiles with such naming scheme are not part of BIDS specification‚Äù*. Si aparece, revisa que `desc-withann` est√© presente y que la ruta quede dentro de `derivatives`.  
* [x] Si el validador emite avisos sobre columnas, re-confirma orden y tipos de `onset`/`duration`. 
---


## üÖ≤ Fase C ¬∑ Detecci√≥n autom√°tica de marcas
(sumar aca como contexto el archivo @marker_correction.py que tiene funciones potencialmente utiles)

* [x] **Implementar algoritmo**
  Crea `scripts/preprocessing/detect_markers.py` y, dentro, usa:

  ```python
  # AUDIO: amplitud
  peaks_audio, _ = find_peaks(audio_data, height=30000)
  # PHOTO: frecuencia
  peaks_photo, _ = find_peaks(photo_data, distance=raw.info['sfreq']/2)
  ```

  `scipy.signal.find_peaks` extrae los picos temporales esperados.

* [x] **Convertir picos a anotaciones**

  ```python
  onsets = peaks_audio / raw.info['sfreq']
  ann_auto = mne.Annotations(onsets, [1.0]*len(onsets), 'auto_marker')
  raw.set_annotations(ann_auto + raw.annotations)
  ```

* [x] **Guardar la versi√≥n autom√°tica**
Anota con description='autoann' para distinguirlo de la versi√≥n manual.
Guarda en derivatives/auto_events/ y crea su propio dataset_description.json.
De nuevo, evita sobrescribir los TSV manuales usando la entidad desc-.

* [x] **Comparar visualmente**

  ```python
  raw_auto = mne_bids.read_raw_bids(deriv_auto_path)
  raw_man  = mne_bids.read_raw_bids(deriv_manual_path)
  raw_man.add_annotations(raw_auto.annotations, emit_warning=False)
  raw_man.pick(['AUDIO','PHOTO']).plot()
  ```

  Decide a ojo cu√°l resulta m√°s fiable y an√≥talo en el log.

---

## üÖ≥ Fase D ¬∑ Registrar observaciones y preparar para extender al resto de los participantes

* [x] Hacer un documento que tenga todos los paths de todos los archivos y sus duraciones
      para poder escalar la forma en que se crean las duraciones de los VIDEOS

duraciones = {
    1: 103,
    2: 229,
    3: 94,
    4: 60,
    5: 81,
    6: 162,
    7: 161,
    8: 77,
    9: 154,
    10: 173,
    11: 103,
    12: 61,
    13: 216,
    14: 116,
    green_intensity_video_1: 60,
    green_intensity_video_3: 60,
    green_intensity_video_7: 60,
    green_intensity_video_9: 60,
    green_intensity_video_12: 60,
}

Estas duraciones fueron extraidas de `C:\Users\Cocudata\experiment_VR\stimuli\exp_videos\VR\DEPRECATED\deprecated_without_whistle`
y de `C:\Users\Cocudata\experiment_VR\stimuli\videos_luminance`.
Considerar que estos videos tienen 6 segundos menos que los presentados (3 segundos en cada extremo extra) 

* [x] Buscar con o3 la mejor forma, de acuerod a BIDS, para codificar a los tipos de 
estimulos que apareceran como description en las anotaciones / eventos
A√±ade un enlace relativo al archivo .tsv dentro del log para trazabilidad.

- [x] Navega a `data/sourcedata/xdf/sub-*/` y abre cada archivo `order_matrix_*_VR.xlsx`
con `pandas.read_excel()` para obtener un `DataFrame`.
- [x] Crear un df la duracion y descripcion de cada video.
      Los onsets inicializarlos en `0`.

Voy a avanzar con 3 archivos (sujeto 16, 17 y 18: 4 bloques cada uno [sin practica])
Despues voy a correr todo esto (incluido el preprocesados con los archivos de todos los participantes que tengo)

Para generar las anotaciones / eventos, corri:
`python scripts/preprocessing/create_events_tsv.py`
Eso corre para una sesion de un participante y genera las anotaciones
En el `main` statement hay que indicar que participante y sesion corremos
Esto lo voy a tener que repetir para todos los aprticipantes (12 a 15)
y tambien con los videos de practica.

- Tengo problema con los participantes 13 y 15. En concreto, falta informacion completa de los .xlsx
files referidos a elm orden de los estimulos, que se generan al generar los videos de cada participante
* 13 : A2, A3, A4 (falta run 1 de A), B2, B4 (falta run 1 y 3 de B)
* 15: Faltan todas las runs, pero en comentarios esta especificado que se uso el de 13.
Solucion
1. Fijarme si es posible reconstruir los runs que faltan generandolos con test=True en Cocudata
2. Si eso no se puede, mirar los videos de los runs que faltan y reconstruir esos archivos a mano
Por ahora voy a seguir sin esas corridas. 
Cuando esos particiapntes esten voy a tener que correr de nuevo `create_events_tsv.py` para 13 y 15

## Extender Fases A a D a todos los archivos del participante 16.
Procura que cada nuevo events.tsv se escriba con description='init' antes de pasar por fases B y C.

Ahora que ya tengo los eventos inicializados para los 4 bloques de
los participantes 16, 17 y 18, lo que tengo que hacer:

- [x] Adaptar `detect_markers.py` para que reciba como input al archivo de eventos de
`./data/derivatives/events` y que me outputee en orden inverso una linea por vez los onset,
duraciones y descripciones de dicho participante, sesion y bloque
(empezando por la ultima linea hasta la primera linea).


Viernes -> empece a revisar  `detect_markers.py` para el S16 sesion 2

Sabado ->


Ya arregle el problema que tenia con la creacion de eventos en create_events_tsv.py
Ahora los eventos creados en `derivatives/events` tienen el orden marker_correctio.
De este modo, ya corri `python scripts/preprocessing/detect_markers.py --subject 16 --session vr --task 02 --run 003 --acq a `
Ya esta abierta la ventana interaciva de mne para afinar las anotaicones.
Y ahora tengo que refinar las anotaciones para que coincidan con el inicio y fin de la marca
Luego de corroborar que eso coincida, pasar al resto de las sesiones del participante 16.


S16 task 01 session 02
----------------------

Recien corri `python scripts/preprocessing/detect_markers.py --subject 16 --session vr --task 01 --run 002 --acq a`

El orden de los eventos deberia ser el siguiente, segun  aparece en el .tsx de events del participante (ver contenido de .tsv adjunto)

No obstante, haciendo inspeccion visual de los eventos, veo que las duraciones no coinciden.
Por lo tanto, los eventos que aparecen en el .tsv parece no ser el correcto

Este es el orden que recontrui viendo las duraciones de los eventos reales en la data, segun los canales AUDIO/PHOTO (el onset no importante por ahora). Es decir, el orden que deberia haber estado tambien en el .tsv de events

onset	duration	trial_type	stim_id	condition	stim_file
0.0	300.0	fixation	500	baseline	stimuli/fixation_cross.mp4
0.0	104.0	calm	901	calm	stimuli/901.mp4
0.0	94.0	video	003	affective	stimuli/3.mp4
0.0	61.028	video	012	affective	stimuli/12.mp4
0.0	229.0	video	002	affective	stimuli/2.mp4
0.0	60.0	video_luminance	112	luminance	stimuli/green_intensity_video_12.mp4

Estoy revisando el .xslx de order_matrix correspondiente a ese participante y sesion y veo que el error se arrastra desde ahi. Ahi ya esta mal el orden.
En concreto, lo que esta sucediendo es que los videos 002 y 003 estan invertidos.
Segun el video, primero viene el video 002 y luego e video 003
Sin embargo, segun el eeg primero vino el 003 y despues el 002.
No entiendo por que sucede este error, pero voy a seguir adelante y ver si con el resto de los
participantes se replica algo de esto.
Esto no se replica en el resto de los participantes, pero es rarisimo.

S16 task 04 session 05
----------------------

Por algun motivo no encuentro la relacion entre las duraciones que deberia tener cada trial con los
eventos encontrados.
Lo que voy a hacer ahora es:
- Revisar el .xslx file de ese archivo y ver si hay queda mas claro que puede estar pasando
  -> Aca no encontre nada, asi que tengo que revisar los videos! 
- Ir a revisar los videos de ese participante, tanto de esta sesion como de la primera, para entender los videos
  -> En los videos veo todo en orden. Seguro haya habido algun problema en el registro del participante.
     Voy a chequear eso ahora
- Si eso no lo resueve, ir a las anotacines .xlsx del participante, a ver si se menciona algo que haya
sucedido durante la toma que explque esto.
  -> Supuestamente en el de la serpiente se paro, pero las duraciones no me coinciden, asi que simplemente
     voy a epochear segun los tiempos donde hay anotaciones y listo

Lo que hice fue segmentear por donde tengo anotaciones.
Los .tsv de merged_events tiene os onset y duraciones correctos, pero los eventos incorrectos.
Voy a avanzar con otros participatnes y ver si asi entiendo qeu es lo que paso.


* Voy a volver a revisar despues la sesion 1 y 4 del sujeto 16, A, cuanto tenga mas data del 
resto de los participantes. La sesion 1 me parece que es como esta en el .tsv de merged_events,
pero la sesion 4 simplemente recorte por donde hay anotaciones de joystick y los nombres de los
eventos y sus duraciones no coinciden, asi que si o si hay que revisarlo despues*

Sujeto 17
---------
Terminado y sin problemas!

Sujeto 18
---------
Las sesiones 2 y 3 no tiene bien las marcas ni de audio ni de photo. Asi que las marcas fueron puestas
a ojo, para que entre toda la anotacion afectiva.
Pero estas sesiones debe excluirse si depues usamos la informacion del video, porque no son precisas.
La ultima sesion (sesion 4, run 5) tiene las marcas correctas

- Ahora voy a seguir procesando los sujetos 12, 13 y 14


Sujeto 12
---------
- task 1 run 2
el tercer estimulo deberia tener duracion de 154.0 segundos, segun lo que aparece en
las anotaciones de `events`. Si embargo, veo una duracion entre marcas de 77.0 segundos,
lo que me da la pauta de que en realidad estamos viendo el 4to estimulo de eventos.
Es decir que en lugar de aparecer este estimulo:
0.0	154.0	video	009	affective	stimuli/9.mp4
aparece este estimulo:
0.0	77.0	video	008	affective	stimuli/8.mp4
Por el contrario, el estimulo que le sigue dura 154.0 segundos, lo que me da la pauta
de que esta invertido tambien.
Esto ya me habia pasado antes y no termino de entender por que para algunos participantes tenemos
estimlos shuffleados.
Despues puedo revisar yendo a los videos y a las anotaciones del participante en el excel.
El orden que tuvo al final fue:
onset	duration	trial_type	stim_id	condition	stim_file
0.0	300.0	fixation	500	baseline	stimuli/fixation_cross.mp4
0.0	104.0	calm	901	calm	stimuli/901.mp4
0.0	77.0	video	008	affective	stimuli/8.mp4
0.0	154.0	video	009	affective	stimuli/9.mp4
0.0	60.027	video	004	affective	stimuli/4.mp4
0.0	161.0	video	007	affective	stimuli/7.mp4
0.0	60.0	video_luminance	103	luminance	stimuli/green_intensity_video_3.mp4

=== Fusionando eventos originales con nuevas anotaciones ===

¬°ADVERTENCIA! Diferencia significativa en la duraci√≥n del evento 3:
  Original: 154.00s
  Nueva: 77.01s
¬°ADVERTENCIA! Diferencia significativa en la duraci√≥n del evento 4:
  Original: 77.00s
  Nueva: 154.02s
¬°ADVERTENCIA! Diferencia significativa en la duraci√≥n del evento 5:
  Original: 161.00s
  Nueva: 60.04s
¬°ADVERTENCIA! Diferencia significativa en la duraci√≥n del evento 6:
  Original: 60.03s
  Nueva: 161.02s

En concreto, ahora los archivos generados en el .tsv del merged_events esta correcto.
Lo que quedo incorrecto es el orden de `events`, posiblemente porque viene incorrecto del 
`order_matrix`.
Solucion parcial: ya revise el video presentado y coincide con los eventos guardados en
`merged_events`

- task 2 run 3
onset	duration	trial_type	stim_id	condition	stim_file
0.0	103.003	video	011	affective	stimuli/11.mp4
0.0	81.014	video	005	affective	stimuli/5.mp4
0.0	103.0	video	001	affective	stimuli/1.mp4
0.0	173.0	video	010	affective	stimuli/10.mp4
0.0	60.0	video_luminance	107	luminance	stimuli/green_intensity_video_7.mp4

El primer estimulo es de 61 segundos, posiblemente estimulo 12.mp4
El segundo estimulo es de 94 segundos, posiblemente estimulo 3.mp4
El tercer estimulo es de 229 segundos, posibelemente estimulo 2.mp4
El 4to estimulo (luminancia) es de 60 segundos

Aparentemente el problema con esto los .xslx de order matrix no eran correctos (estaban generados 2 veces)
Ya reemplace en mi carpeta de data para la sesion A los archivos con los order matrix correcto.
El problema con esto es que no se si la sesion B va a estar incorrecto (o, lo que puede pasar
tambien es que no tengamos reportes de arousal Y valenica para cada sujeto, si fue genrado dos veces)


-----
Termine el sujeto 14 tambien. Y voy a avanzar con los analisis para los sujetos 14, 16, 17 y 18 (unica sesion por ahora)
Para el sujeto 14, el task 2 run 3 no se puede obtener las marcas, por algun motivo.

-----
Establecer que antes de cada toma se chequee que este correctamente guardados todos los .xslx
asi como tambien todos los videos.
Si eso no esta, agarrar los datos del participante anterior.



Martes ->

- [x] Revisar todas las se√±ales (anoaciones, fisiologia, etc) del ultimo participnate que tomo Jero con la data de EDA
      Anotar si hay cuestiones generales del experimento a modificar antes de continuar con la toma 
      e.g. forma en que guardo los datos (nombre de los files de acuerdo a BIDS), forms del experimento incluyendo sexo, etc.

Lo corri de este modo:
`python scripts/sanity_check/test_check_physiology_manual.py --subject 16 --task 01 --run 003 --acq a
`
Asi note que la se√±al de EDA para este participante no esta bien.
ECG y Respiracion esta bien.


- [x] Configurar Arete en la PC que me dieron
 https://nicobruno.notion.site/Documentation-COCUCO-6c010b140dfc4496a47ed94b5c6917d3?pvs=74

- First Data Pipeline -> correr esto local

SEGUIR DESDE ACA ->
- Sacar notas de preprocesamiento del paper de protocolo de GLHMM


- En base al script de nico de preprocesamiento, armar un script para preprocesar un paerticipante, sesion y tarea (i.e. un archivo de dato)


    - [ ] Crear un script reproducible de preprocesamiento EEG para un participante:
        1. Leer datos BIDS de un sujeto/sesi√≥n/tarea.
        2. Filtrar (bandpass) y notch.
        3. Detecci√≥n autom√°tica y visual de canales ruidosos.
        4. Segmentar en epochs seg√∫n eventos.
        5. Rechazo autom√°tico y manual de epochs (AutoReject).
        6. ICA + clasificaci√≥n autom√°tica de componentes (ICLabel).
        7. Interpolaci√≥n de canales malos y rereferencia.
        8. Guardar epochs preprocesados y reporte HTML en `data/derivatives/`.
        9. Loggear todos los pasos y par√°metros en JSON.


##  üÖ¥ Fase E Epoching
* Antes de empezar con esto, revisar los papers de Denis de meeglet a ver como necesito la data
(asi no trabajo de mas). Considerar preguntarle a Denis, llegado el caso, si seria posible obtener los power spectrum
time series con morlet wavelets a partir de meeglet (o si eso es posible simplemente para freq y no time freq analysis).

* [ ] **Definir ventanas y crear Epochs**
- El fichero epochs (_epo.fif) es claramente un derivado; col√≥calo en derivatives/epochs/ y usa la entidad desc-epo o, si te resulta m√°s informativo, suffix='epo' con datatype='eeg'.
- Genera otro dataset_description.json para esa carpeta con GeneratedBy apuntando al script de epoching.

  ```python
  events, event_id = mne.events_from_annotations(raw)  # manual o auto, seg√∫n elija el log
  epochs = mne.Epochs(raw, events, event_id, tmin, tmax,
                      baseline=None, preload=True)
  ```

* [ ] **Guardar como derivado ‚Äúepochs‚Äù**

  ```python
  epo_root = bids_root / 'derivatives' / 'epochs'
  epo_path = BIDSPath(subject='16', session='vr', task='practice',
                      run='001', suffix='epo', extension='.fif',
                      description='epo', root=epo_root)
  epochs.save(epo_path, overwrite=True)
  ```

* [ ] **Describir el pipeline de epochs**
  Ejecuta `mne_bids.make_dataset_description()` otra vez, esta vez apuntando a `derivatives/epochs` con `dataset_type='derivative'`.

* [ ] **Validar y versionar**

  ```bash
  bids-validator derivatives/epochs
  dvc add derivatives/epochs
  git commit -m "Add epochs derivation"

Miercoles
---------

- [ ] Chequear la data del participante 16 cond B (ya tengo los datos en  `./data/sourcedata/xdf)
      - [x] Lo primeero que tengo qeu hacer es hacer read_xdf. Listo!
      - [x] Insepccionar con `test_check_physiology_manual`
      - [x] Correr `create_events_tsv` para estas tasks de este sujeto 16
      - [x] Despues detect_markers

      Todos los bloques de la sesion b del participante 16 tiene las marcas de audio/photo bien SALVO el
      prmer bloque. Este bloque tiene una duracion menor de la esperada, y las marcas no coinciden con
      los tiempos de `events`.

      SEGUIR DESDE ACA ->
      - Revisar en xochipili si no quedo algun archivo de eeg borrador,
        como supo pasar alguna otra vez. Ahora no lo encuentro, asi que simplemente voy a 
        empezar con las otras tasks y saltear esta.
      - Si no aparece, voy a proceder descartando el task 1 de sujeto 16 (b).

Jueves ->

- [ ] Hablar con Diego para reunirnos ma√±ana

- [ ] Chequear la data del participante 14 cond b
      - [ ] Descargarlos y llevarlos a  `./data/sourcedata/xdf
      - [ ] Lo primeero que tengo qeu hacer es hacer read_xdf. 
      - [ ] Insepccionar con `test_check_physiology_manual`
      - [ ] Correr `create_events_tsv` para estas tasks de este sujeto 16
      - [ ] Despues detect_markers

- [ ] Preprocesar ahora solo sujeto 14 cond B (si algo de esto no funciono bien, ir al 16 cond B e toda la data bien, que ya empece a preprocesar)
      - Si me trabo habla con nico

- [ ] Primera implementacion de GLHMM, de acuerdo a lo que conversamos con Diego


Viernes ->

- Preprocesar el EEG de todos los paticipantes hoy (14, 16, 17,18).
- Correr los analisis con todos esos paraticipantes


Queda para despues ->

- Estrucutrar los datos de las encuestas (por ahora guardados simplemente en `sourcedata` como 3 csvs separados)
    - [ ] Poblar el archivo `participants.tsv` con la informacion de los participantes.
          Por ahora esto es un .tsv vacio, porque la funcion update_participant_info() esta comentada.
          Pero deberia utuluzadar la data de las encuestas para poder extraer esa informacion
    - [ ] Agregar el resto de la informacion (e.g. escalas) como `phenotype` o similar,
          segun recoemndado por BIDS para estructurar encuestas, etc.


- Bajar la data en Arete