Semana que viene ->

Martes ->

- [x] Terminar de editar read_xdf /o bids compliance para asegurar que se este agregando bien
      la informacion de lso participantes a la metadata.
- [x] Una vez terminado eso (que ya lo pregunte en ctrl+l), fijarme a ver como incorporar el resto 
      de la data de los participantes. Primero, lo tengo que agregar en la sourcedata. Y luego
      lo tengo que sumar a la `./raw` de acuerdo a BIDS
 
Miercoles -> Hacer los pasos 1, 2 y 3 de "Fases y salidas" (@arquitectura_datos)
- [x] Localizar las marcas de inicio y fin de registro. 
      La primera marca no estaba, porque parece que se hacia justo al comenzar el registro y quedaba por fuera
      La ultima marca aparecia pero justo al limite del registro, asi que no la agregamos.
- [x] Confirmar que la estructura BIDS es correcta usando BIDS Validator
- [x] Sacar toda la parte de sesiones con nox, linters automaticos, etc al momento de commitear.
      Tengo que poder pushear sin pasar por todos los filtros que tengo ahora (fue mucha complejidad).
      Eliminar tambien del pipeline el paso por PR de GitHub. Dejar todo esto para milestones del proyecto nomas. 
- [x] Docuemntar y pushear todos los cambios
- [x] Conversar con Jero como seguir. Explicarle como generar los archivos de cada participante.
      Y que la idea seria generar un script que epochee de acuerdo a las marcas.

Jueves -> Preparacion de Reunion con Diego (preguntas a responder con modelos GLHMM, etc)

Viernes ->  Prerparcion de tareas para inspecci√≥n general en los registros de los sujetos 


Sabado -> Inspecci√≥n general en los registros de los sujetos (principalemente las marcas, para poder chunkear los datos)

## üÖ∞Ô∏è Fase A ¬∑ Construir el `events.tsv` inicial a partir de crear un archivo en `./scripts/preprocessing`

* [ ] **Localizar la planilla de √≥rdenes**

Vamos a empezar levantando el archivo 

* [x] **Filtrar filas relevantes**
  Conserva solo las que tengan `description` en `['fixation', 'calm_901', 'video', 'luminance', 'verbal_report', 'calm_902']` usando `df.query()`.

* [x] **Asignar duraciones**
  Crea el diccionario Python de onset y durations.
  Para eso, vamos a tomar como input el siguiente dict con start y end de cada evento.
  ```
     durations = {'video': [0:00:35, 0:03:18],
                  'video': [0:05:29, 0:09:04],
                  'video': [0:11:17, 0:13:13], 
                  'luminance': [0:15:32, 0:16:30]}
  ```

* [x] **Inicializar anotaciones ‚Äúblandas‚Äù**

  ```python
  onsets = np.zeros(len(df))
  annot  = mne.Annotations(onsets, df.description.map(durations),
                           df.description)
  raw.set_annotations(annot)
  ```

  `mne.Annotations` garantiza compatibilidad posterior con `events_from_annotations()`.

* [x] **Exportar a BIDS**
  Construye un `BIDSPath` que apunte a `data/raw/sub-XX/ses-vr/..._eeg.eeg` y llama:

  ```python
  mne_bids.write_raw_bids(raw, bids_path,
                          events=annot,
                          overwrite=True)
  ```

  Esto crea autom√°ticamente `*_events.tsv` y  `*_events.json` en derivatives

* [x] **Validar**
  Ejecuta en terminal:

  ```bash
  bids-validator data/derivatives
  ```

  Corrige cualquier *ERROR* o *WARNING* antes de continuar.

---

## üÖ±Ô∏è Fase B ¬∑ Ajustar onsets con AUDIO y PHOTO

* [x] **Cargar eventos existentes y raw original**
  
  ```python
  from mne_bids import read_raw_bids, BIDSPath
  
  # Ruta a los eventos generados
  events_path = BIDSPath(subject='16', session='vr', task='02',
                        run='003', suffix='events', desc='init', extension='.tsv',
                        root=deriv_root, check=False)
  
  events_df = pd.read_csv(events_path.fpath, sep='\t')
  
  # Cargar raw original SIN anotaciones
  raw = read_raw_bids(bids_path)
  ```

* [x] **Visualizar marcadores sensoriales vs eventos**

  ```python
  # Crear anotaciones desde events.tsv
  annot = mne.Annotations(onset=events_df['onset'],
                         duration=events_df['duration'],
                         description=events_df['trial_type'])
  
  # Visualizar alineaci√≥n
  raw.pick(['AUDIO', 'PHOTO']).set_annotations(annot).plot()
  ```
VIDEOS
('0:00:35', '0:03:18') -> 203,141 a 365,292
('0:05:29', '0:09:04') -> 497,074 a 713,234
('0:11:17', '0:13:13') -> 1006,567 a 1123,834
LUMINANCE
('0:15:32', '0:16:30') -> 1261,240 a 1321,293

Lunes -> Terminar el dia con todos los archivos con las anotaciones guardadas en derivatives.
Si hay tiempo, empezar al pipeline de preprocesamiento de EEG.

* [x] Correr `visualize_events.py`, guardar las 4 anotaciones en los tiempos que corresponden
      (ver numeros mas arriba).

* [x] Con otro script, verificar si eso se guardo bien (`verify_annotations.py`)

## ‚òëÔ∏è Mejorar el archivo `visualize_events` para que se haga el guardado en *derivatives/aligned\_events*

* [x] **BIDSPath completo con `desc-withann`** (la entidad `desc-` distingue esta versi√≥n del TSV ‚Äúprivilegiado‚Äù y evita alertas de validador):

  ```python
bids_path = BIDSPath(subject='16', session='vr', task='02', run='003',
                  suffix='eeg', extension='.vhdr', datatype='eeg', root=bids_root)

  ```

* [x] Graba el TSV con separador `\t` y sin √≠ndice.
* [x] **Copia el `.json` del TSV original** y a√±ade/actualiza:
  ```json
  {
    "Description": "Eventos alineados con se√±ales fisiol√≥gicas",
    "Sources": ["../aligned_events/../desc-init_events.tsv"],
    "OffsetApplied": "manual AUDIO/PHOTO (s)
  }
  ```

  Campos adicionales son permitidos en sidecars JSON. 

## ‚òëÔ∏èMejorar el archivo `visualize_events` en cuanto a sus metadatos de la carpeta derivada

* [x] Ejecuta una sola vez por carpeta:

  ```python
  mne_bids.make_dataset_description(
      path=aligned_root,
      name='aligned_events',
      dataset_type='derivative',
      generated_by=[{
          "Name": "W3_phaseB.py",
          "Description": "Alineaci√≥n manual de eventos AUDIO/PHOTO"
      }],
      source_datasets=[{
          "URL": str(events_path.fpath)  # ruta relativa al TSV base
      }]
  )
  ```

  Esto cumple con la obligaci√≥n de `dataset_description.json` propia del pipeline. 

## ‚òëÔ∏è Validaci√≥n final

* [x] Corre `bids-validator derivatives/aligned_events` y confirma que **no aparezca** la advertencia *‚ÄúFiles with such naming scheme are not part of BIDS specification‚Äù*. Si aparece, revisa que `desc-withann` est√© presente y que la ruta quede dentro de `derivatives`.  
* [x] Si el validador emite avisos sobre columnas, re-confirma orden y tipos de `onset`/`duration`. 
---


## üÖ≤ Fase C ¬∑ Detecci√≥n autom√°tica de marcas
(sumar aca como contexto el archivo @marker_correction.py que tiene funciones potencialmente utiles)

* [x] **Implementar algoritmo**
  Crea `scripts/preprocessing/detect_markers.py` y, dentro, usa:

  ```python
  # AUDIO: amplitud
  peaks_audio, _ = find_peaks(audio_data, height=30000)
  # PHOTO: frecuencia
  peaks_photo, _ = find_peaks(photo_data, distance=raw.info['sfreq']/2)
  ```

  `scipy.signal.find_peaks` extrae los picos temporales esperados.

* [x] **Convertir picos a anotaciones**

  ```python
  onsets = peaks_audio / raw.info['sfreq']
  ann_auto = mne.Annotations(onsets, [1.0]*len(onsets), 'auto_marker')
  raw.set_annotations(ann_auto + raw.annotations)
  ```

* [x] **Guardar la versi√≥n autom√°tica**
Anota con description='autoann' para distinguirlo de la versi√≥n manual.
Guarda en derivatives/auto_events/ y crea su propio dataset_description.json.
De nuevo, evita sobrescribir los TSV manuales usando la entidad desc-.

* [x] **Comparar visualmente**

  ```python
  raw_auto = mne_bids.read_raw_bids(deriv_auto_path)
  raw_man  = mne_bids.read_raw_bids(deriv_manual_path)
  raw_man.add_annotations(raw_auto.annotations, emit_warning=False)
  raw_man.pick(['AUDIO','PHOTO']).plot()
  ```

  Decide a ojo cu√°l resulta m√°s fiable y an√≥talo en el log.

---

## üÖ≥ Fase D ¬∑ Registrar observaciones y preparar para extender al resto de los participantes

* [x] Hacer un documento que tenga todos los paths de todos los archivos y sus duraciones
      para poder escalar la forma en que se crean las duraciones de los VIDEOS

duraciones = {
    1: 103,
    2: 229,
    3: 94,
    4: 60,
    5: 81,
    6: 162,
    7: 161,
    8: 77,
    9: 154,
    10: 173,
    11: 103,
    12: 61,
    13: 216,
    14: 116,
    green_intensity_video_1: 60,
    green_intensity_video_3: 60,
    green_intensity_video_7: 60,
    green_intensity_video_9: 60,
    green_intensity_video_12: 60,
}

Estas duraciones fueron extraidas de `C:\Users\Cocudata\experiment_VR\stimuli\exp_videos\VR\DEPRECATED\deprecated_without_whistle`
y de `C:\Users\Cocudata\experiment_VR\stimuli\videos_luminance`.
Considerar que estos videos tienen 6 segundos menos que los presentados (3 segundos en cada extremo extra) 

* [x] Buscar con o3 la mejor forma, de acuerod a BIDS, para codificar a los tipos de 
estimulos que apareceran como description en las anotaciones / eventos
A√±ade un enlace relativo al archivo .tsv dentro del log para trazabilidad.

- [x] Navega a `data/sourcedata/xdf/sub-*/` y abre cada archivo `order_matrix_*_VR.xlsx`
con `pandas.read_excel()` para obtener un `DataFrame`.
- [x] Crear un df la duracion y descripcion de cada video.
      Los onsets inicializarlos en `0`.

Voy a avanzar con 3 archivos (sujeto 16, 17 y 18: 4 bloques cada uno [sin practica])L
Despues voy a correr todo esto (incluido el preprocesados con los archivos de todos los participantes que tengo)

Para generar las anotaciones / eventos, corri:
`python scripts/preprocessing/create_events_tsv.py`
Eso corre para una sesion de un participante y genera las anotaciones
En el `main` statement hay que indicar que participante y sesion corremos
Esto lo voy a tener que repetir para todos los aprticipantes (12 a 15)
y tambien con los videos de practica.

- Tengo problema con los participantes 13 y 15. En concreto, falta informacion completa de los .xlsx
files referidos a elm orden de los estimulos, que se generan al generar los videos de cada participante
* 13 : A2, A3, A4 (falta run 1 de A), B2, B4 (falta run 1 y 3 de B)
* 15: Faltan todas las runs, pero en comentarios esta especificado que se uso el de 13.
Solucion
1. Fijarme si es posible reconstruir los runs que faltan generandolos con test=True en Cocudata
2. Si eso no se puede, mirar los videos de los runs que faltan y reconstruir esos archivos a mano
Por ahora voy a seguir sin esas corridas. 
Cuando esos particiapntes esten voy a tener que correr de nuevo `create_events_tsv.py` para 13 y 15

## Extender Fases A a D a todos los archivos del participante 16.
Procura que cada nuevo events.tsv se escriba con description='init' antes de pasar por fases B y C.

Ahora que ya tengo los eventos inicializados para los 4 bloques de
los participantes 16, 17 y 18, lo que tengo que hacer:

- [x] Adaptar `detect_markers.py` para que reciba como input al archivo de eventos de
`./data/derivatives/events` y que me outputee en orden inverso una linea por vez los onset,
duraciones y descripciones de dicho participante, sesion y bloque
(empezando por la ultima linea hasta la primera linea).





Jueves -> 

- [ ] Conectarme a Arete con las credenciales que me paso Nico ->
 https://nicobruno.notion.site/Documentation-COCUCO-6c010b140dfc4496a47ed94b5c6917d3?pvs=74

 - [ ] Correr detect_markers (con ajustes manuales) con la corrida 2 del participante 16
* Terminar esto que no llegue a correrlo ayer

##  üÖ¥ Fase E Epoching
* Antes de empezar con esto, revisar los papers de Denis de meeglet a ver como necesito l:a data
(asi no trabajo de mas)

* [ ] **Definir ventanas y crear Epochs**
- El fichero epochs (_epo.fif) es claramente un derivado; col√≥calo en derivatives/epochs/ y usa la entidad desc-epo o, si te resulta m√°s informativo, suffix='epo' con datatype='eeg'.
- Genera otro dataset_description.json para esa carpeta con GeneratedBy apuntando al script de epoching.

  ```python
  events, event_id = mne.events_from_annotations(raw)  # manual o auto, seg√∫n elija el log
  epochs = mne.Epochs(raw, events, event_id, tmin, tmax,
                      baseline=None, preload=True)
  ```


* [ ] **Guardar como derivado ‚Äúepochs‚Äù**

  ```python
  epo_root = bids_root / 'derivatives' / 'epochs'
  epo_path = BIDSPath(subject='16', session='vr', task='practice',
                      run='001', suffix='epo', extension='.fif',
                      description='epo', root=epo_root)
  epochs.save(epo_path, overwrite=True)
  ```

* [ ] **Describir el pipeline de epochs**
  Ejecuta `mne_bids.make_dataset_description()` otra vez, esta vez apuntando a `derivatives/epochs` con `dataset_type='derivative'`.

* [ ] **Validar y versionar**

  ```bash
  bids-validator derivatives/epochs
  dvc add derivatives/epochs
  git commit -m "Add epochs derivation"
  ```


- First Data Pipeline
(EEG Preprocessing, segun lo hacen para papers que usan meeglet, ver codigo de Denis)
    - [ ] Cargar pipeline EEG en un sujeto
        - Verificar funcionamiento b√°sico sin errores
        - Anotar requerimientos de ajustes
    - [ ] Crear un script reproducible de preprocesamiento EEG para un participante:
        1. Leer datos BIDS de un sujeto/sesi√≥n/tarea.
        2. Filtrar (bandpass) y notch.
        3. Detecci√≥n autom√°tica y visual de canales ruidosos.
        4. Segmentar en epochs seg√∫n eventos.
        5. Rechazo autom√°tico y manual de epochs (AutoReject).
        6. ICA + clasificaci√≥n autom√°tica de componentes (ICLabel).
        7. Interpolaci√≥n de canales malos y rereferencia.
        8. Guardar epochs preprocesados y reporte HTML en `data/derivatives/`.
        9. Loggear todos los pasos y par√°metros en JSON.


- [ ] Primera implementacion de GLHMM, de acuerdo a lo que conversamos con Diego


Viernes -> Continuar con los runs que faltan del sujeto 16 y con sujetos 17 y 18.
*Esto hacerlo cuando ya tenga acceso al server, con el menor decim que pueda (idealmente menor que 4)
Volver a correr el run 2 del participante 16 con mejor decim en server.

Correr detect markers
- [ ] Luego pasar a la corrida 3, 4 y 1 del participante 16 de "detect_markers".
(en ese orden). Ir ajustando a mano para que coincidan los tiempos del estimulo con los tiempos
entre eventos. Esa debe ser mi chequeo. Solo grbaar si estoy seguro del resultado.
- [ ] Completar participantes 17 y 18

- [ ] Hacer epocheo
- [ ] Preprocesar datos de EEG y correr pipelines de Denis para extraer Morlet wavelet en estos participantes
- [ ] Hacer implementacion de GLHMM tambien para estos participantes


Queda para despues ->

- [ ] Revisar todas las se√±ales (anoaciones, fisiologia, etc)
      Anotar si hay cuestiones generales del experimento a modificar antes de continuar con la toma 
      e.g. forma en que guardo los datos (nombre de los files de acuerdo a BIDS), forms del experimento incluyendo sexo, etc.

- Estrucutrar los datos de las encuestas (por ahora guardados simplemente en `sourcedata` como 3 csvs separados)
    - [ ] Poblar el archivo `participants.tsv` con la informacion de los participantes.
          Por ahora esto es un .tsv vacio, porque la funcion update_participant_info() esta comentada.
          Pero deberia utuluzadar la data de las encuestas para poder extraer esa informacion
    - [ ] Agregar el resto de la informacion (e.g. escalas) como `phenotype` o similar,
          segun recoemndado por BIDS para estructurar encuestas, etc.